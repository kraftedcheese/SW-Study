{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Using Pushshift Module to extract Submissions.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-QQANye3ecE"
      },
      "source": [
        "# Using Pushshift Module to extract Submissions Data from Reddit via Python\n",
        "\n",
        "PRAW is pretty good at gettin reddit data but there are some limitations with it.\n",
        "Including the removal of the [subreddit.submissions endpoint](https://www.reddit.com/r/changelog/comments/7tus5f/update_to_search_api/.). \n",
        "\n",
        "So for extracting Reddit submissions and the primarily data such as upvotes and comments count, I put together this notebook using Pushshift.\n",
        "\n",
        "If you still prefer PRAW for extract submissions, I have written a code [template here](https://github.com/SeyiAgboola/Seyi_Projects/blob/master/submission_list.py).\n",
        "\n",
        "I will also [host the code on GitHub](https://github.com/SeyiAgboola/Reddit-Data-Mining/blob/master/Using_Pushshift_Module_to_extract_Submissions.ipynb).\n",
        "\n",
        "More info on the removal of the [subreddit.submissions endpoint](https://www.reddit.com/r/redditdev/comments/8bia9n/praw_psa_the_subredditsubmissions_method_no/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyarnetivJPG"
      },
      "source": [
        "# Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfmIce345UaB"
      },
      "source": [
        "import pandas as pd\n",
        "import requests #Pushshift accesses Reddit via an url so this is needed\n",
        "import json #JSON manipulation\n",
        "import csv #To Convert final table into a csv file to save to your machine\n",
        "import time\n",
        "import datetime"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj8JGor4vMwC"
      },
      "source": [
        "# Pushshift URL Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH8nB39CzZfU"
      },
      "source": [
        "#We can access the Pushshift API through building an URL with the relevant parameters without even needing Reddit credentials.\n",
        "#These are some examples. You can follow the links and they will generate a page with JSON data\n",
        "search_ps4_after_date = \"https://api.pushshift.io/reddit/search/submission/?q=screenshot&after=1514764800&before=1517443200&subreddit=PS4\"\n",
        "search_science = \"https://api.pushshift.io/reddit/search/submission/?q=science\""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFPXbeD-0kGc"
      },
      "source": [
        "# Parameters for your Pushshift URL\n",
        "These are probably the most important parameters to consider when building your Pushshift URL:\n",
        "\n",
        "* size — increase limit of returned entries to 1000\n",
        "* after — where to start the search\n",
        "* before — where to end the search\n",
        "* title — to search only within the submission’s title\n",
        "* subreddit — to narrow it down to a particular subreddit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDma-H_k0frf"
      },
      "source": [
        "#Adapted from this https://gist.github.com/dylankilkenny/3dbf6123527260165f8c5c3bc3ee331b\n",
        "#This function builds an Pushshift URL, accesses the webpage and stores JSON data in a nested list\n",
        "def getPushshiftData(query, after, before, sub):\n",
        "    #Build URL\n",
        "    #url = 'https://api.pushshift.io/reddit/search/submission/?title='+str(query)+'&size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
        "    url = 'https://api.pushshift.io/reddit/search/submission/?'+'size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)\n",
        "    #Print URL to show user\n",
        "    print(url)\n",
        "    #Request URL\n",
        "    r = requests.get(url)\n",
        "    #Load JSON data from webpage into data variable\n",
        "    try:\n",
        "        data = json.loads(r.text)\n",
        "    except:\n",
        "        return None\n",
        "    #return the data element which contains all the submissions data\n",
        "    return data['data']"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H524E6kkvh1O"
      },
      "source": [
        "# Extract key information from Submissions\n",
        "\n",
        "We want key data for further analysis including: \n",
        "* Submission Title\n",
        "* URL \n",
        "* Flair\n",
        "* Author\n",
        "* Submission post ID\n",
        "* Score\n",
        "* Upload Time\n",
        "* No. of Comments \n",
        "* Permalink.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6qX7glQ1-nl"
      },
      "source": [
        "#This function will be used to extract the key data points from each JSON result\n",
        "def collectSubData(subm):\n",
        "    #subData was created at the start to hold all the data which is then added to our global subStats dictionary.\n",
        "    subData = list() #list to store data points\n",
        "    title = subm['title']\n",
        "    url = subm['url']\n",
        "    #flairs are not always present so we wrap in try/except\n",
        "    try:\n",
        "        flair = subm['link_flair_text']\n",
        "    except KeyError:\n",
        "        flair = \"NaN\"    \n",
        "    author = subm['author']\n",
        "    sub_id = subm['id']\n",
        "    score = subm['score']\n",
        "    try:\n",
        "        selftext = subm['selftext']\n",
        "    except KeyError:\n",
        "        selftext = \"NaN\"\n",
        "    created = datetime.datetime.fromtimestamp(subm['created_utc']) #1520561700.0\n",
        "    numComms = subm['num_comments']\n",
        "    permalink = subm['permalink']\n",
        "    #Put all data points into a tuple and append to subData\n",
        "    subData.append((sub_id,title,url,author,score,created,numComms,permalink,flair, selftext))\n",
        "    #Create a dictionary entry of current submission data and store all data related to it\n",
        "    subStats[sub_id] = subData"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCDRrn9nwRsj"
      },
      "source": [
        "# Update your Search Settings here"
      ]
    },
    {
      "source": [
        "suicidewatch_2020 - after: 1577836800, before: 1607266725, query = \"\", sub = \"SuicideWatch\"   \n",
        "suicidewatch_2019 - after: 1546300800, before: 1577836799, query = \"\", sub = \"SuicideWatch\"  \n",
        "suicidewatch_2019_2 - after: 1562233200, before: 1577836799"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0fNU7eS2mwT"
      },
      "source": [
        "#Create your timestamps and queries for your search URL\n",
        "#https://www.unixtimestamp.com/index.php > Use this to create your timestamps\n",
        "after = \"1562233200\" #Submissions after this timestamp (1577836800 = 01 Jan 20)\n",
        "before = \"1577836799\" #Submissions before this timestamp (1607040000 = 04 Dec 20)\n",
        "query = \"\" #Keyword(s) to look for in submissions\n",
        "sub = \"SuicideWatch\" #Which Subreddit to search in\n",
        "\n",
        "#subCount tracks the no. of total submissions we collect\n",
        "subCount = 0\n",
        "#subStats is the dictionary where we will store our data.\n",
        "subStats = {}"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wP_j8pp2-M8",
        "tags": []
      },
      "source": [
        "# We need to run this function outside the loop first to get the updated after variable\n",
        "data = getPushshiftData(query, after, before, sub)\n",
        "# Will run until all posts have been gathered i.e. When the length of data variable = 0\n",
        "# from the 'after' date up until before date\n",
        "if data is not None:\n",
        "    while len(data) > 0: #The length of data is the number submissions (data[0], data[1] etc), once it hits zero (after and before vars are the same) end\n",
        "        for submission in data:\n",
        "            collectSubData(submission)\n",
        "            subCount+=1\n",
        "        # Calls getPushshiftData() with the created date of the last submission\n",
        "        print(len(data))\n",
        "        print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
        "        #update after variable to last created date of submission\n",
        "        after = data[-1]['created_utc']\n",
        "        #data has changed due to the new after variable provided by above code\n",
        "        data_temp = getPushshiftData(query, after, before, sub)\n",
        "        i = 0\n",
        "        while data_temp is None:\n",
        "            print(\"error in getting data, retrying\")\n",
        "            after = data[-1]['created_utc'] + i\n",
        "            data_temp = getPushshiftData(query, after, before, sub)\n",
        "            i += 1\n",
        "        data = data_temp\n",
        "    \n",
        "    print(len(data))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1562233200&before=1577836799&subreddit=SuicideWatch\n",
            "100\n",
            "2019-07-05 02:22:52\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1562264572&before=1577836799&subreddit=SuicideWatch\n",
            "100\n",
            "2019-07-05 09:17:13\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1562289433&before=1577836799&subreddit=SuicideWatch\n",
            "100\n",
            "2019-07-05 15:23:58\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1562311438&before=1577836799&subreddit=SuicideWatch\n",
            "100\n",
            "2019-07-05 23:48:08\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1562341688&before=1577836799&subreddit=SuicideWatch\n",
            "100\n",
            "2019-07-06 07:43:42\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1562370222&before=1577836799&subreddit=SuicideWatch\n",
            "100\n",
            "2019-07-06 14:29:45\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1562394585&before=1577836799&subreddit=SuicideWatch\n",
            "error in getting data, retrying\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1562394585&before=1577836799&subreddit=SuicideWatch\n",
            "100\n",
            "2019-07-07 00:46:09\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1562431569&before=1577836799&subreddit=SuicideWatch\n",
            "100\n",
            "2019-07-07 08:59:46\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1562461186&before=1577836799&subreddit=SuicideWatch\n",
            "100\n",
            "2019-07-07 15:24:16\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1562484256&before=1577836799&subreddit=SuicideWatch\n",
            "100\n",
            "2019-07-08 02:26:19\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1562523979&before=1577836799&subreddit=SuicideWatch\n",
            "100\n",
            "2019-07-08 08:51:30\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1562547090&before=1577836799&subreddit=SuicideWatch\n",
            "100\n",
            "2019-07-08 13:46:59\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1562564819&before=1577836799&subreddit=SuicideWatch\n",
            "100\n",
            "2019-07-08 21:53:58\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1562594038&before=1577836799&subreddit=SuicideWatch\n",
            "100\n",
            "2019-07-09 05:08:34\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1562620114&before=1577836799&subreddit=SuicideWatch\n",
            "100\n",
            "2019-07-09 10:55:26\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1562640926&before=1577836799&subreddit=SuicideWatch\n",
            "100\n",
            "2019-07-09 17:59:18\n",
            "https://api.pushshift.io/reddit/search/submission/?size=1000&after=1562666358&before=1577836799&subreddit=SuicideWatch\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ChunkedEncodingError",
          "evalue": "('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36m_update_chunk_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    684\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 16: b''",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36m_error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    424\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m                 \u001b[1;32myield\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    751\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_chunk_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36m_update_chunk_length\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    688\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 689\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mhttplib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mIncompleteRead\u001b[0m: IncompleteRead(0 bytes read)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    749\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mstream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchunked\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupports_chunked_reads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_chunked\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    780\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_response\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 781\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    782\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type, value, traceback)\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36m_error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    442\u001b[0m                 \u001b[1;31m# This includes IncompleteRead.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mProtocolError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Connection broken: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mProtocolError\u001b[0m: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-21-bcf88b255118>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mafter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'created_utc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m#data has changed due to the new after variable provided by above code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mdata_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetPushshiftData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbefore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mdata_temp\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-18-4a548f242fdc>\u001b[0m in \u001b[0;36mgetPushshiftData\u001b[1;34m(query, after, before, sub)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#Request URL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;31m#Load JSON data from webpage into data variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    531\u001b[0m         }\n\u001b[0;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m             \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mcontent\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    826\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34mb''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content_consumed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    751\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 753\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    754\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mDecodeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mContentDecodingError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mChunkedEncodingError\u001b[0m: ('Connection broken: IncompleteRead(0 bytes read)', IncompleteRead(0 bytes read))"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaeTGu7mwyoU"
      },
      "source": [
        "# Check your Submission Extraction was successful"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVLuSJ8e8p7i"
      },
      "source": [
        "print(str(len(subStats)) + \" submissions have added to list\")\n",
        "print(\"1st entry is:\")\n",
        "print(list(subStats.values())[0][0][1] + \" created: \" + str(list(subStats.values())[0][0][5]))\n",
        "print(\"Last entry is:\")\n",
        "print(list(subStats.values())[-1][0][1] + \" created: \" + str(list(subStats.values())[-1][0][5]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48500 submissions have added to list\n1st entry is:\nI screwed up my life created: 2019-01-01 08:08:19\nLast entry is:\nI am trapped in my own mistakes created: 2019-07-04 09:39:53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAm_zZZfw521"
      },
      "source": [
        "# Save data to CSV file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBikywNJ8ufl"
      },
      "source": [
        "def updateSubs_file():\n",
        "    upload_count = 0\n",
        "    #location = \"\\\\Reddit Data\\\\\" >> If you're running this outside of a notebook you'll need this to direct to a specific location\n",
        "    print(\"input filename of submission file, please add .csv\")\n",
        "    filename = input() #This asks the user what to name the file\n",
        "    file = filename\n",
        "    with open(file, 'w', newline='', encoding='utf-8') as file: \n",
        "        a = csv.writer(file, delimiter=',')\n",
        "        headers = [\"Post ID\",\"Title\",\"Url\",\"Author\",\"Score\",\"Publish Date\",\"Total No. of Comments\",\"Permalink\",\"Flair\", \"SelfText\"]\n",
        "        a.writerow(headers)\n",
        "        for sub in subStats:\n",
        "            a.writerow(subStats[sub][0])\n",
        "            upload_count+=1\n",
        "            \n",
        "        print(str(upload_count) + \" submissions have been uploaded\")\n",
        "updateSubs_file()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input filename of submission file, please add .csv\n",
            "48500 submissions have been uploaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}